\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\usepackage{amsmath}
\graphicspath{{media/}}     % organize your images and other figures under media/ folder

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{Comparative Analysis of Diffusion Models for Image Generation}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{Comparative Analysis of Diffusion Models for Image Generation
%%%% Cite as
%%%% Update your official citation here when published 
%\thanks{\textit{\underline{Citation}}: 
%\textbf{V. Sharan. Comparative Analysis of Diffusion Models for Image Generation.}} 
}

\author{
  Vaibhav Sharan\\
  \texttt{vsharan1@asu.edu} \\ 
  \and
  Krishnaprasad Palamattam Aji\\
  \texttt{kpalamat@asu.edu}\\
  \and
  Unnikrishnan Madhavan\\
  \texttt{umadhava@asu.edu}\\
  \and
  Ansh Sharma\\
  \texttt{ansh@asu.edu}
}


\begin{document}
\maketitle

\section{Introduction}
The project aims at implementing and comparing different diffusion models that are used for 
image generation. We will evaluate the performance of these different models in generating
such high-quality images from textual prompts. The well known models like FLUX, Stable
Diffusion will be studied in detail and compared with each other on the basis of
their strengths, weaknesses and applicability in different scenarios. This study aims to shed light
on the effectiveness of text to image diffusion models.


\section{Background and Motivation}
Artificial Intelligence has witnessed a paradigm shift in techniques for image generation over the past few years. 
Generative Adversarial Networks(GANs) have been the go to approach for synthetic image creation for a long time. 
Diffusion models recently emerged as a powerful alternative, especially in the domain of text-to-image conversion \cite{dhariwal2021}. 

Diffusion models were introduced in 2015 by Sohl-Dickstein et al.\cite{sohl2015} and have gained popularity 
due to their ability to generate diverse images with high quality and remarkable fidelity. GANs rely on a 
generator-discriminator (adversarial) architecture to generate images. Unlike GANs, diffusion models use a 
gradual denoising process to generate images which has shown remarkable stability during training and better control
over the generation process.

Diffusion models gained popularity due to their exceptional performance in generating images from text. Models like DALL-E 2,
Stable Diffusion, FLUX.1 and Midjourney have captured public imagination with their ability to form real like images 
from textual descriptions. Our project "Comparative Analysis of Diffusion Models for Image Generation", is motivated by the 
need to understand the strengths and weaknesses of different diffusion model architectures. These models are evolving rapidly, and a 
comprehensive comparison is crucial and beneficial to both researchers and practitioners in this field.

To conduct this comparison and analysis, we will be using a range of quantitative metrics that have become a standard 
in evaluating such image generation models. The metrics we use will include Inception Score(IS), Fréchet Inception Distance(FID),
Contrastive Language-Image Pre-training(CLIP), Text-to-Image Faithfulness evaluation with question Answering(TIFA), and 
CLIP Maximum Mean Discrepancy(CMMD). 

By utilizing these metrics, we aim to provide a nuanced understanding of how different diffusion models fare across different aspects
of image generation. This will help researchers and practitioners in the field to select the right diffusion model for their work. 



\section{Related Work}

The evaluation of diffusion models for image generation is an active area of research with different metrics to assess the various
aspects of images generated. The key metrics we will be using in our comparison are:

\subsection{Inception Score (IS)}

Introduced by Salimans et al. \cite{salimans2016improved}, the Inception Score measures the quality and diversity of the images generated.
It was initially designed for GANs but it can be used for any image generation models. It uses a pre-trained Inception v3 network to 
evaluate how well the generated images can be classified into distinct categories. The IS is calculated as:

\begin{equation}
    IS = \exp(\mathbb{E}_x[KL(p(y|x) || p(y))])
\end{equation}

where $p(y|x)$ is the conditional label distribution for generated images, and $p(y)$ is the marginal label distribution over all generated images.
A higher IS score tells us that the image is more diverse and realistic.


\subsection{Fréchet Inception Distance (FID)}

Proposed by Heusel et al. \cite{heusel2017gans}, FID is a combination of Fréchet distance and features extracted from the 
Inception-v3 model. FID was introduced after IS and addresses some of its limitations.
FID calculates the distance between the feature representations of the generated and real image distributions:

\begin{equation}
    FID = ||\mu_r - \mu_g||^2 + Tr(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2})
\end{equation}

where $\mu_r, \Sigma_r$ are the mean and covariance of the real image features, and $\mu_g, \Sigma_g$ are those of the generated images.
 Lower FID scores suggest that the generated images are closer to real images in terms of quality and diversity.

\subsection{CLIP Score}

Leveraging the CLIP (Contrastive Language-Image Pre-training) model developed by Radford et al. \cite{radford2021learning}, 
the CLIP Score assesses how well images are generated and align with their text prompts:

\begin{equation}
    CLIP\_Score = \cos(CLIP_{text}(prompt), CLIP_{image}(generated\_image))
\end{equation}

where $\cos$ is the cosine similarity between the CLIP embeddings of the text prompt and the generated image.
A higher CLIP score means that the generated image matches better with the text prompt.

\subsection{CLIP-Maximum Mean Discrepancy}

CLIP-MMD(CMMD) is an extension of the CLIP score and uses Maximum Mean Discrepancy (MMD) to measure the distance between the 
CLIP embeddings of generated and real images \cite{gao2022measuring}:

\begin{equation}
    CLIP\text{-}MMD = MMD(CLIP_{image}(real\_images), CLIP_{image}(generated\_images))
\end{equation}

A lower CMMD score means that the generated images are closer to the real images in terms of feature space distribution. CMMD claims to 
fix some of the limitations of FID \cite{rethinkingFID2024}.

\subsection{TIFA (Text-to-Image Faithfulness evaluation with question Answering)}

Introduced by Yang et al. \cite{yang2022empirical}, TIFA measures how faitfully a generated image represents its text prompt.
It uses visual question answering to evaluate how well the generated image captures the information from their textual descriptions:

\begin{equation}
    TIFA\_Score = Accuracy(VQA_{model}(generated\_image, question\_from\_prompt))
\end{equation}

where the VQA model answers questions derived from the original text prompt based on the generated image. 
TIFA is a fairly new metric and compared to other traditional metrics, offers a more fine-grained assessment of text-image alignment.

When these metrics are used in combination to evaluate our diffusion models, we can assess the quality, diversity and faithfulness
to text prompts of the images generated.


\section{Progress}


\section{Execution Plan}

\section{Workload Distribution}

\section{Headings: first level}
\label{sec:headings}

\lipsum[4] See Section \ref{sec:headings}.

\subsection{Headings: second level}
\lipsum[5]
\begin{equation}
\xi _{ij}(t)=P(x_{t}=i,x_{t+1}=j|y,v,w;\theta)= {\frac {\alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}{\sum _{i=1}^{N} \sum _{j=1}^{N} \alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}}
\end{equation}

\subsubsection{Headings: third level}
\lipsum[6]

\paragraph{Paragraph}

\lipsum[7]

\section{Examples of citations, figures, tables, references}
\label{sec:others}
\lipsum[8] \cite{dhariwal2021,kour2014fast} and see \cite{hadash2018estimate}.

The documentation for \verb+natbib+ may be found at
\begin{center}
  \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}
Of note is the command \verb+\citet+, which produces citations
appropriate for use in inline text.  For example,
\begin{verbatim}
   \citet{hasselmo} investigated\dots
\end{verbatim}
produces
\begin{quote}
  Hasselmo, et al.\ (1995) investigated\dots
\end{quote}

\begin{center}
  \url{https://www.ctan.org/pkg/booktabs}
\end{center}


\subsection{Figures}
\lipsum[10] 
See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}
\lipsum[11] 

\begin{figure}
  \centering
  \fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
  \label{fig:fig1}
\end{figure}

\subsection{Tables}
\lipsum[12]
See awesome Table~\ref{tab:table}.

\begin{table}
 \caption{Sample table title}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
  \label{tab:table}
\end{table}

\subsection{Lists}
\begin{itemize}
\item Lorem ipsum dolor sit amet
\item consectetur adipiscing elit. 
\item Aliquam dignissim blandit est, in dictum tortor gravida eget. In ac rutrum magna.
\end{itemize}


\section{Conclusion}
Your conclusion here

\section*{Acknowledgments}
This was was supported in part by......

%Bibliography
\bibliographystyle{unsrt}  
\bibliography{references}  


\end{document}
