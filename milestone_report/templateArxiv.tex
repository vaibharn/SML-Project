\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\usepackage{amsmath}
\graphicspath{{media/}}     % organize your images and other figures under media/ folder

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{Comparative Analysis of Diffusion Models for Image Generation}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{Comparative Analysis of Diffusion Models for Image Generation
%%%% Cite as
%%%% Update your official citation here when published 
%\thanks{\textit{\underline{Citation}}: 
%\textbf{V. Sharan. Comparative Analysis of Diffusion Models for Image Generation.}} 
}

\author{
  Vaibhav Sharan\\
  \texttt{vsharan1@asu.edu} \\ 
  \and
  Krishnaprasad Palamattam Aji\\
  \texttt{kpalamat@asu.edu}\\
  \and
  Unnikrishnan Madhavan\\
  \texttt{umadhava@asu.edu}\\
  \and
  Ansh Sharma\\
  \texttt{ansh@asu.edu}
}


\begin{document}
\maketitle

\section{Introduction}
The project aims at implementing and comparing different diffusion models that are used for 
image generation. We will evaluate the performance of these different models in generating
such high-quality images from textual prompts. The well known models like FLUX, Stable
Diffusion will be studied in detail and compared with each other on the basis of
their strengths, weaknesses and applicability in different scenarios. This study aims to shed light
on the effectiveness of text to image diffusion models.


\section{Background and Motivation}
Artificial Intelligence has witnessed a paradigm shift in techniques for image generation over the past few years. 
Generative Adversarial Networks(GANs) have been the go to approach for synthetic image creation for a long time. 
Diffusion models recently emerged as a powerful alternative, especially in the domain of text-to-image conversion \cite{dhariwal2021}. 

Diffusion models were introduced in 2015 by Sohl-Dickstein et al.\cite{sohl2015} and have gained popularity 
due to their ability to generate diverse images with high quality and remarkable fidelity. GANs rely on a 
generator-discriminator (adversarial) architecture to generate images. Unlike GANs, diffusion models use a 
gradual denoising process to generate images which has shown remarkable stability during training and better control
over the generation process.

Diffusion models gained popularity due to their exceptional performance in generating images from text. Models like DALL-E 2,
Stable Diffusion, FLUX.1 and Midjourney have captured public imagination with their ability to form real like images 
from textual descriptions. Our project "Comparative Analysis of Diffusion Models for Image Generation", is motivated by the 
need to understand the strengths and weaknesses of different diffusion model architectures. These models are evolving rapidly, and a 
comprehensive comparison is crucial and beneficial to both researchers and practitioners in this field.

To conduct this comparison and analysis, we will be using a range of quantitative metrics that have become a standard 
in evaluating such image generation models. The metrics we use will include Inception Score(IS), Fr√©chet Inception Distance(FID),
Contrastive Language-Image Pre-training(CLIP), Text-to-Image Faithfulness evaluation with question Answering(TIFA), and 
CLIP Maximum Mean Discrepancy(CMMD). 

By utilizing these metrics, we aim to provide a nuanced understanding of how different diffusion models fare across different aspects
of image generation. This will help researchers and practitioners in the field to select the right diffusion model for their work. 



\section{Related Work}
Recent advancements in diffusion models have focused on improving various aspects such as efficiency, scale, 
architecture, training techniques, and controllability. This comparative study aims to evaluate several state-of-the-art 
diffusion-based text-to-image models across various metrics to assess their strengths, limitations, and potential synergies in the 
domain of text-to-image generation. This section provides an overview of key developments in diffusion-based text-to-image synthesis, 
with a focus on three prominent models: Stable Diffusion, FLUX.1, and Kwai Kolors.

Stable Diffusion, developed by Rombach et al. \cite{kulal2024stablediffusion}, introduced the concept of latent diffusion, allowing for efficient training 
and inference while maintaining high image quality. Stable diffusion focuses on improving and 
scaling rectified  flow models for high-resolution text-to-image synthesis. A rectified flow formulation, which connects 
data and noise in a straight 
line, rather than traditional curved diffusion paths is used. A set of new noise samplers (Logit-Normal sampling, Mode sampling 
with heavy tails, CosMap sampling) that improve performance over previous samplers were introduced for rectified flow models. 
A novel transformer-based architecture called MM-DiT (Multimodal Diffusion Transformer) specifically designed for text-to-image 
tasks was used in stable diffusion. It uses separate weights for text and image modalities. Larger models can be sampled using fewer 
steps. For example, their depth=38 model shows only a 2.71\% relative CLIP score decrease when using 5 sampling steps instead of 50, 
compared to 4.30\% for the depth=15 model. They demonstrate a strong correlation between validation loss and comprehensive 
evaluation metrics like GenEval, T2I-CompBench, and human preference ratings.

FLUX.1, a more recent development by Black Forest labs, aims to improve upon Stable Diffusion's capabilities by incorporating 
advanced techniques such as rectified flow and parallel attention layers \cite{flux2024medium}. FLUX claims to offer enhanced speed, efficiency, 
and prompt adherence compared to earlier diffusion models. All public FLUX.1 models are based on a hybrid architecture of multimodal 
and parallel diffusion transformer blocks and scaled to 12B parameters. The model claims to improve over previous state-of-the-art diffusion 
models by building on flow matching, a general and conceptually simple method for training generative models, which includes 
diffusion as a special case \cite{flux2024main}. In addition, the model claims to have increased model performance and improved hardware efficiency by 
incorporating rotary positional embeddings and parallel attention layers.

Kwai-Kolors is a large-scale text-to-image generation model based on latent diffusion, developed by the Kuaishou Kolors team. 
Similar to other latent diffusion models, Kolors operates in a compressed latent space rather than pixel space, 
which offers advantages in computational efficiency and generation quality \cite{kolors2024}. The model was trained on billions of text-image 
pairs and demonstrates significant capabilities in visual quality, complex semantic accuracy, and text rendering for both 
Chinese and English characters. The Kolors model is based on a latent diffusion architecture, which operates in a compressed 
latent space rather than pixel space. This approach offers several advantages such as reduced computational complexity, improved 
generation quality, and enhanced robustness to noise. the model architecture consists of three main components: a variational encoder 
(VAE) for encoding and ddecoding images, a U-Net back bone for the diffusion process, and a noise-aware classifier-free guidance 
mechanism which allows for better control over the generation process and improved quality of generated images. Kolors employs an 
adaptive noise schedule that adjusts the noise level based on the input image quality. By operating in a compressed latent space, 
Kolors achieves faster training and inference times compared to pixel-space diffusion models. This feature enhances the model's robustness 
to various levels of noise.

The below table captures some of the characteristics of these models:

\begin{table}
  \centering
  \begin{tabular}{p{1.5in} p{1.5in} p{1.5in} p{1.5in}}
    \toprule
    \textbf{Model} & \textbf{Stable Diffusion} & \textbf{FLUX.1} & \textbf{Kwai-Kolors} \\
    \midrule
    \textbf{Architecture} & Latent diffusion model & Rectified flow Transformer & Latent diffusion model \\
    \textbf{Parameter Size} & 860 million (SD 1.5) & 12 billion & 2.6 billion \\
    \textbf{Image Quality} & High & Very high, especially with dev/pro versions & High \\
    \textbf{Prompt Understanding} & Keyword-based & Advanced NLP & Advanced NLP \\
    \textbf{Unique Features} & Inpainting, outpainting, image-to-image & Excellent prompt adherence, detailed outputs & High-quality photorealistic synthesis, noise robust \\
    \bottomrule
  \end{tabular}
  \label{tab:comparison}
  \caption{Comparison of Stable Diffusion, FLUX.1, and Kwai-Kolors}
\end{table}

\section{Evaluation Metrics}
The evaluation of diffusion models for image generation is an active area of research with different metrics to assess the various
aspects of images generated. The key metrics we will be using in our comparison are:

\subsection{Inception Score (IS)}

Introduced by Salimans et al. \cite{salimans2016improved}, the Inception Score measures the quality and diversity of the images generated.
It was initially designed for GANs but it can be used for any image generation models. It uses a pre-trained Inception v3 network to 
evaluate how well the generated images can be classified into distinct categories. The IS is calculated as:

\begin{equation}
    IS = \exp(\mathbb{E}_x[KL(p(y|x) || p(y))])
\end{equation}

where $p(y|x)$ is the conditional label distribution for generated images, and $p(y)$ is the marginal label distribution over all generated images.
A higher IS score tells us that the image is more diverse and realistic.


\subsection{Fr√©chet Inception Distance (FID)}

Proposed by Heusel et al. \cite{heusel2017gans}, FID is a combination of Fr√©chet distance and features extracted from the 
Inception-v3 model. FID was introduced after IS and addresses some of its limitations.
FID calculates the distance between the feature representations of the generated and real image distributions:

\begin{equation}
    FID = ||\mu_r - \mu_g||^2 + Tr(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2})
\end{equation}

where $\mu_r, \Sigma_r$ are the mean and covariance of the real image features, and $\mu_g, \Sigma_g$ are those of the generated images.
 Lower FID scores suggest that the generated images are closer to real images in terms of quality and diversity.

\subsection{CLIP Score}

Leveraging the CLIP (Contrastive Language-Image Pre-training) model developed by Radford et al. \cite{radford2021learning}, 
the CLIP Score assesses how well images are generated and align with their text prompts:

\begin{equation}
    CLIP\_Score = \cos(CLIP_{text}(prompt), CLIP_{image}(generated\_image))
\end{equation}

where $\cos$ is the cosine similarity between the CLIP embeddings of the text prompt and the generated image.
A higher CLIP score means that the generated image matches better with the text prompt.

\subsection{CLIP-Maximum Mean Discrepancy}

CLIP-MMD(CMMD) is an extension of the CLIP score and uses Maximum Mean Discrepancy (MMD) to measure the distance between the 
CLIP embeddings of generated and real images \cite{gao2022measuring}:

\begin{equation}
    CLIP\text{-}MMD = MMD(CLIP_{image}(real\_images), CLIP_{image}(generated\_images))
\end{equation}

A lower CMMD score means that the generated images are closer to the real images in terms of feature space distribution. CMMD claims to 
fix some of the limitations of FID \cite{rethinkingFID2024}.

\subsection{TIFA (Text-to-Image Faithfulness evaluation with question Answering)}

Introduced by Yang et al. \cite{yang2022empirical}, TIFA measures how faitfully a generated image represents its text prompt.
It uses visual question answering to evaluate how well the generated image captures the information from their textual descriptions:

\begin{equation}
    TIFA\_Score = Accuracy(VQA_{model}(generated\_image, question\_from\_prompt))
\end{equation}

where the VQA model answers questions derived from the original text prompt based on the generated image. 
TIFA is a fairly new metric and compared to other traditional metrics, offers a more fine-grained assessment of text-image alignment.

When these metrics are used in combination to evaluate our diffusion models, we can assess the quality, diversity and faithfulness
to text prompts of the images generated.


\section{Progress}
For the Milestone 1 Report, we are about one week behind schedule as mentioned the timeline we estimated in the Project proposal because of more extensive literature review than expected. We have included the updated Execution Plan accordingly such that we are able to finish the project in stipulated time. The following subsections describe the tasks we have completed for the first milestone.

\subsection{Survey of State of the Art Diffusion Models}
We performed a comprehensive search and survey of the current open-source models available on the internet, more specifically on websites like HuggingFace.co, Github, civit.ai to identify the models we are going to perform this comparative study on. We selected FLUX.1[dev] by Black Forest Lab, Stable Diffusion 3 Medium by StabilityAI, and Kolors by Kuaishou Kolors.

\subsection{Identification of Evaluation Metrics}
As mentioned in the above section, we are going to use Inception Score, Fr√©chet Inception Distance, CLIP Score, CLIP-MMD, and TIFA for our comparative study.

\subsection{Environment Setup for the Diffusion Models}
We have created and activated virtual environments for each of the three models with all the required packages and libraries including PyTorch, Hugging Face Diffusers, CUDA. The model weights are ready to use for the evaluation step.




\section{Execution Plan}
This following timeline is the updated version taking into consideration the work done till first milestone.

\subsection{Prompt Engineering (Week 6)}
We will be creating sample text prompts for image generation in different categories such as
people, multiple objects, landscapes, portraits.

\subsection{Collection of Evaluation Metrics Data (Weeks 7 and 8)}
In this phase, we will use identical prompts to generate images from all models and
analyze them to evaluate the generated image output. We will implement and collect metrics for quantitative evaluation as discussed before.

\subsection{Comparative Analysis (Week 9)}
During this part of project, we will use different metrics and compare models on them along with assessing their performance by testing various batch sizes and image resolutions.

\subsection{Results Compilation and Conclusions (Week 10)}
Now we will start writing reports by collecting and analyzing the results data and perform
statistical analysis to highlight significant differences between models. We will also use
graphs and charts to further elaborate our results.

\subsection{Report Writing and Presentation (Weeks 11 and 12)}
Here, we will write a detailed report on highlighting results, and conclusions and simultaneously develop a presentation, we will also include a gallery of generated images for comparison. In the last week, an internal peer review will be conducted to finalize the report and presentation based on everyone‚Äôs feedback.



\section{Workload Distribution}

\subsection{Model Setup and Implementation (Vaibhav Sharan, Ansh Sharma)}
\begin{itemize}
    \item Set up local environments for Stable Diffusion, FLUX.1, and Kwai-Kolors
    \item Implement image generation scripts for each model
\end{itemize}

\subsection{Quantitative Metrics Implementation (Krishnaprasad Palamattam Aji)}
\begin{itemize}
    \item Develop scripts for IS, FID, CLIP Score and CMMD
\end{itemize}

\subsection{Data Collection and Processing (Unnikrishnan Madhavan)}
\begin{itemize}
    \item Create diverse text prompts dataset
    \item Develop scripts for TIFA metrics
\end{itemize}

\subsection{Analysis and Report Writing (All Team Members)}
\begin{itemize}
    \item Analyze results and compare model performances
    \item Write findings and prepare visualizations
\end{itemize}

Weekly meetings will be held to track progress and address challenges.


%Bibliography
\bibliographystyle{unsrt}  
\bibliography{references}


\section{Headings: first level}
\label{sec:headings}

\lipsum[4] See Section \ref{sec:headings}.

\subsection{Headings: second level}
\lipsum[5]
\begin{equation}
\xi _{ij}(t)=P(x_{t}=i,x_{t+1}=j|y,v,w;\theta)= {\frac {\alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}{\sum _{i=1}^{N} \sum _{j=1}^{N} \alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}}
\end{equation}

\subsubsection{Headings: third level}
\lipsum[6]

\paragraph{Paragraph}

\lipsum[7]

\section{Examples of citations, figures, tables, references}
\label{sec:others}
\lipsum[8] \cite{dhariwal2021,kour2014fast} and see \cite{hadash2018estimate}.

The documentation for \verb+natbib+ may be found at
\begin{center}
  \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}
Of note is the command \verb+\citet+, which produces citations
appropriate for use in inline text.  For example,
\begin{verbatim}
   \citet{hasselmo} investigated\dots
\end{verbatim}
produces
\begin{quote}
  Hasselmo, et al.\ (1995) investigated\dots
\end{quote}

\begin{center}
  \url{https://www.ctan.org/pkg/booktabs}
\end{center}


\subsection{Figures}
\lipsum[10] 
See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}
\lipsum[11] 

\begin{figure}
  \centering
  \fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
  \label{fig:fig1}
\end{figure}

\subsection{Tables}
\lipsum[12]
See awesome Table~\ref{tab:table}.

\begin{table}
 \caption{Sample table title}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
  \label{tab:table}
\end{table}

\subsection{Lists}
\begin{itemize}
\item Lorem ipsum dolor sit amet
\item consectetur adipiscing elit. 
\item Aliquam dignissim blandit est, in dictum tortor gravida eget. In ac rutrum magna.
\end{itemize}


\section{Conclusion}
Your conclusion here

\section*{Acknowledgments}
This was was supported in part by......


%Bibliography
\bibliographystyle{unsrt}  
\bibliography{references}  


\end{document}
